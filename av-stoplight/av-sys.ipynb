{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install roboflow\n",
    "%pip install inference-sdk\n",
    "%pip install torch\n",
    "%pip install torchvision\n",
    "\n",
    "%pip install ultralytics==8.0.196\n",
    "%pip install pyyaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import yaml\n",
    "import torchvision.transforms as T\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"o2Vk0Lj1qpKf8uGx9lZJ\")\n",
    "project = rf.workspace(\"bolbol-duawh\").project(\"trafic_lights_detection\")\n",
    "version = project.version(3)\n",
    "dataset = version.download(\"yolov8\")\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TrafficLightDataset(Dataset):\n",
    "    def __init__(self, root_dir, split, transforms=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.split = split  # 'train', 'test', or 'valid'\n",
    "        self.transforms = transforms\n",
    "        \n",
    "        split_dir = os.path.join(root_dir, split)\n",
    "        self.img_dir = os.path.join(split_dir, \"images\")\n",
    "        self.label_dir = os.path.join(split_dir, \"labels\")\n",
    "        \n",
    "        self.imgs = list(sorted(os.listdir(self.img_dir)))\n",
    "        self.labels = list(sorted(os.listdir(self.label_dir)))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
    "        label_path = os.path.join(self.label_dir, self.labels[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        target = self.parse_label(label_path)\n",
    "        \n",
    "        if self.transforms is not None:\n",
    "            img, target = self.transforms(img, target)\n",
    "        \n",
    "        # Ensure boxes is always 2D\n",
    "        if target['boxes'].ndim == 1:\n",
    "            target['boxes'] = target['boxes'].unsqueeze(0)\n",
    "        \n",
    "        # Ensure labels is always 1D\n",
    "        target['labels'] = target['labels'].view(-1)\n",
    "        \n",
    "        return img, target\n",
    "\n",
    "    def parse_label(self, label_path):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(label_path, 'r') as f:\n",
    "            for line in f:\n",
    "                values = line.strip().split()\n",
    "                class_id = int(values[0])\n",
    "                labels.append(class_id)\n",
    "                x_center, y_center, width, height = map(float, values[1:])\n",
    "                x_min = x_center - width / 2\n",
    "                y_min = y_center - height / 2\n",
    "                x_max = x_center + width / 2\n",
    "                y_max = y_center + height / 2\n",
    "                boxes.append([x_min, y_min, x_max, y_max])\n",
    "        \n",
    "        # Handle empty labels\n",
    "        if not boxes:\n",
    "            boxes.append([0, 0, 1, 1])  # Add a dummy box\n",
    "            labels.append(0)  # Add a dummy label (background)\n",
    "        \n",
    "        return {\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.int64)\n",
    "        }\n",
    "\n",
    "print(\"Class Traffic Light Dataset completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Compose:\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        for t in self.transforms:\n",
    "            image, target = t(image, target)\n",
    "        return image, target\n",
    "\n",
    "class ToTensor(object):\n",
    "    def __call__(self, image, target):\n",
    "        image = F.to_tensor(image)\n",
    "        return image, target\n",
    "\n",
    "class RandomHorizontalFlip(object):\n",
    "    def __init__(self, prob):\n",
    "        self.prob = prob\n",
    "\n",
    "    def __call__(self, image, target):\n",
    "        if random.random() < self.prob:\n",
    "            image = F.hflip(image)\n",
    "            bbox = target[\"boxes\"]\n",
    "            bbox[:, [0, 2]] = 1 - bbox[:, [2, 0]]\n",
    "            target[\"boxes\"] = bbox\n",
    "        return image, target\n",
    "\n",
    "def get_transform(train):\n",
    "    transforms = []\n",
    "    transforms.append(ToTensor())\n",
    "    if train:\n",
    "        transforms.append(RandomHorizontalFlip(0.5))\n",
    "    return Compose(transforms)\n",
    "\n",
    "print('Class Compose completed successfully')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn, FasterRCNN_ResNet50_FPN_Weights\n",
    "\n",
    "def get_model(num_classes):\n",
    "    weights = FasterRCNN_ResNet50_FPN_Weights.DEFAULT\n",
    "    model = fasterrcnn_resnet50_fpn(weights=weights)\n",
    "    \n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    \n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    \n",
    "    print(f\"Model configured for {num_classes} classes\")  # Add this line\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_model(model, data_loader, optimizer, device, num_epochs):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        num_batches = 0\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            try:\n",
    "                images = list(image.to(device) for image in images)\n",
    "                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "                \n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += losses.item()\n",
    "                num_batches += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error during training: {e}\")\n",
    "                for i, t in enumerate(targets):\n",
    "                    print(f\"Sample {i}:\")\n",
    "                    print(f\"  Boxes shape: {t['boxes'].shape}\")\n",
    "                    print(f\"  Labels shape: {t['labels'].shape}\")\n",
    "                continue\n",
    "        \n",
    "        if num_batches > 0:\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "        else:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}, No valid batches\")\n",
    "\n",
    "print(\"model training completed successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        with open('./trafic_lights_detection-3/data.yaml', 'r') as file:\n",
    "            config = yaml.safe_load(file)\n",
    "        \n",
    "        print(f\"Number of classes in config: {config['nc']}\")  # Add this line\n",
    "        print(f\"Class names: {config['names']}\")\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Create datasets for train, validation, and test\n",
    "        train_dataset = TrafficLightDataset(root_dir='./trafic_lights_detection-3', split='train', transforms=get_transform(train=True))\n",
    "        val_dataset = TrafficLightDataset(root_dir='./trafic_lights_detection-3', split='valid', transforms=get_transform(train=False))\n",
    "        test_dataset = TrafficLightDataset(root_dir='./trafic_lights_detection-3', split='test', transforms=get_transform(train=False))\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "        val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "        test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))\n",
    "\n",
    "        # Create model\n",
    "        num_classes = config['nc'] + 1  # Add 1 for the background class\n",
    "        model = get_model(num_classes=num_classes)\n",
    "        model.to(device)\n",
    "\n",
    "        # Optimizer\n",
    "        params = [p for p in model.parameters() if p.requires_grad]\n",
    "        optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "\n",
    "        # Train\n",
    "        num_epochs = 10  # You can adjust this or make it a parameter\n",
    "        try:\n",
    "            train_model(model, train_loader, optimizer, device, num_epochs=num_epochs)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during training: {e}\")\n",
    "\n",
    "        # Save model\n",
    "        torch.save(model.state_dict(), 'traffic_light_model.pth')\n",
    "        print(\"Training completed and model saved successfully.\")\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print(f\"Error: {e}. Please check if the data.yaml file exists in the specified location.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n",
    "    print(\"main ran succesffuly, model has been trained\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
